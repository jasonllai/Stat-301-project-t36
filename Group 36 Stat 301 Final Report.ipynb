{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8de639f6-523a-4dcd-9b52-f5d17e2ca4d7",
   "metadata": {},
   "source": [
    "# **STAT 301 Project Final Report**\n",
    "\n",
    "\n",
    "### <font color=red> Online News Popularity\n",
    "#### -  Bokai Lai, Sophia Oh, Can Okten, Rita Zhuang"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2437932a-7cc9-4536-bd88-8cd835d50b0d",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a615d5d3-6e59-4c6b-a8bd-5efa034f81cb",
   "metadata": {},
   "source": [
    "In recent times, digital platforms have been primarily used as a source of news. According to Pew Research Center, 86% of U.S. adults often or sometimes access digital platforms for news, and among digital platforms, news websites and apps are most preferred. An existing scientific publication has used shareworthiness, a characteristic of online content to spread quickly, considering geographic proximity, cultural distance, and positivity/negativity of content to predict shares (Trilling et al., 2017). However, our group chose the “Online News Popularity” dataset from UCI Machine Learning Repository（https://archive.ics.uci.edu/dataset/332/online+news+popularity), which includes features about articles published by the global news website Mashable. There are many factors influencing how the public receives and shares content, and our goal is to build an effective model to predict the number of shares an article gets based on its characteristics.\n",
    "\n",
    "\n",
    "Articles on Mashable can be directly shared on Facebook, Twitter, and Flipboard.  \tThe dataset contains articles that Mashable published between 2013 and 2015. There are 39,797 observations and 61 variables. To answer a prediction question, we will analyze shares as the response variable. Shares are an integer between 0 and infinite, representing the overall popularity and reach of an article."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c213db-86b1-4d6a-b5f3-58be8bb6e8cf",
   "metadata": {},
   "source": [
    "# Methods and Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de171ad9-86cf-4ffc-b344-f7681be20c67",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7bae6cea-003e-4c06-92b5-1adaa795f83e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updating HTML index of packages in '.Library'\n",
      "\n",
      "Making 'packages.html' ...\n",
      " done\n",
      "\n"
     ]
    }
   ],
   "source": [
    "install.packages(\"Hmisc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4c20c79-22ec-495c-8fbc-13ab580d106a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updating HTML index of packages in '.Library'\n",
      "\n",
      "Making 'packages.html' ...\n",
      " done\n",
      "\n"
     ]
    }
   ],
   "source": [
    "install.packages(\"psych\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57a5cd1-0fcb-4765-8bb3-b22494e11f27",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "── \u001b[1mAttaching core tidyverse packages\u001b[22m ──────────────────────── tidyverse 2.0.0 ──\n",
      "\u001b[32m✔\u001b[39m \u001b[34mdplyr    \u001b[39m 1.1.3     \u001b[32m✔\u001b[39m \u001b[34mreadr    \u001b[39m 2.1.4\n",
      "\u001b[32m✔\u001b[39m \u001b[34mforcats  \u001b[39m 1.0.0     \u001b[32m✔\u001b[39m \u001b[34mstringr  \u001b[39m 1.5.0\n",
      "\u001b[32m✔\u001b[39m \u001b[34mggplot2  \u001b[39m 3.4.3     \u001b[32m✔\u001b[39m \u001b[34mtibble   \u001b[39m 3.2.1\n",
      "\u001b[32m✔\u001b[39m \u001b[34mlubridate\u001b[39m 1.9.3     \u001b[32m✔\u001b[39m \u001b[34mtidyr    \u001b[39m 1.3.0\n",
      "\u001b[32m✔\u001b[39m \u001b[34mpurrr    \u001b[39m 1.0.2     \n",
      "── \u001b[1mConflicts\u001b[22m ────────────────────────────────────────── tidyverse_conflicts() ──\n",
      "\u001b[31m✖\u001b[39m \u001b[34mdplyr\u001b[39m::\u001b[32mfilter()\u001b[39m masks \u001b[34mstats\u001b[39m::filter()\n",
      "\u001b[31m✖\u001b[39m \u001b[34mdplyr\u001b[39m::\u001b[32mlag()\u001b[39m    masks \u001b[34mstats\u001b[39m::lag()\n",
      "\u001b[36mℹ\u001b[39m Use the conflicted package (\u001b[3m\u001b[34m<http://conflicted.r-lib.org/>\u001b[39m\u001b[23m) to force all conflicts to become errors\n",
      "\n",
      "Attaching package: ‘cowplot’\n",
      "\n",
      "\n",
      "The following object is masked from ‘package:lubridate’:\n",
      "\n",
      "    stamp\n",
      "\n",
      "\n",
      "Registered S3 method overwritten by 'GGally':\n",
      "  method from   \n",
      "  +.gg   ggplot2\n",
      "\n",
      "Loading required package: carData\n",
      "\n",
      "\n",
      "Attaching package: ‘car’\n",
      "\n",
      "\n",
      "The following object is masked from ‘package:dplyr’:\n",
      "\n",
      "    recode\n",
      "\n",
      "\n",
      "The following object is masked from ‘package:purrr’:\n",
      "\n",
      "    some\n",
      "\n",
      "\n",
      "── \u001b[1mAttaching packages\u001b[22m ────────────────────────────────────── tidymodels 1.1.1 ──\n",
      "\n",
      "\u001b[32m✔\u001b[39m \u001b[34mdials       \u001b[39m 1.2.0     \u001b[32m✔\u001b[39m \u001b[34mtune        \u001b[39m 1.1.2\n",
      "\u001b[32m✔\u001b[39m \u001b[34mmodeldata   \u001b[39m 1.2.0     \u001b[32m✔\u001b[39m \u001b[34mworkflows   \u001b[39m 1.1.3\n",
      "\u001b[32m✔\u001b[39m \u001b[34mparsnip     \u001b[39m 1.1.1     \u001b[32m✔\u001b[39m \u001b[34mworkflowsets\u001b[39m 1.0.1\n",
      "\u001b[32m✔\u001b[39m \u001b[34mrecipes     \u001b[39m 1.0.8     \u001b[32m✔\u001b[39m \u001b[34myardstick   \u001b[39m 1.2.0\n",
      "\u001b[32m✔\u001b[39m \u001b[34mrsample     \u001b[39m 1.2.0     \n",
      "\n",
      "── \u001b[1mConflicts\u001b[22m ───────────────────────────────────────── tidymodels_conflicts() ──\n",
      "\u001b[31m✖\u001b[39m \u001b[34mscales\u001b[39m::\u001b[32mdiscard()\u001b[39m masks \u001b[34mpurrr\u001b[39m::discard()\n",
      "\u001b[31m✖\u001b[39m \u001b[34mdplyr\u001b[39m::\u001b[32mfilter()\u001b[39m   masks \u001b[34mstats\u001b[39m::filter()\n",
      "\u001b[31m✖\u001b[39m \u001b[34mrecipes\u001b[39m::\u001b[32mfixed()\u001b[39m  masks \u001b[34mstringr\u001b[39m::fixed()\n",
      "\u001b[31m✖\u001b[39m \u001b[34mdplyr\u001b[39m::\u001b[32mlag()\u001b[39m      masks \u001b[34mstats\u001b[39m::lag()\n",
      "\u001b[31m✖\u001b[39m \u001b[34mcar\u001b[39m::\u001b[32mrecode()\u001b[39m     masks \u001b[34mdplyr\u001b[39m::recode()\n",
      "\u001b[31m✖\u001b[39m \u001b[34mcar\u001b[39m::\u001b[32msome()\u001b[39m       masks \u001b[34mpurrr\u001b[39m::some()\n",
      "\u001b[31m✖\u001b[39m \u001b[34myardstick\u001b[39m::\u001b[32mspec()\u001b[39m masks \u001b[34mreadr\u001b[39m::spec()\n",
      "\u001b[31m✖\u001b[39m \u001b[34mrecipes\u001b[39m::\u001b[32mstep()\u001b[39m   masks \u001b[34mstats\u001b[39m::step()\n",
      "\u001b[34m•\u001b[39m Dig deeper into tidy modeling with R at \u001b[32mhttps://www.tmwr.org\u001b[39m\n",
      "\n",
      "Loading required package: Matrix\n",
      "\n",
      "\n",
      "Attaching package: ‘Matrix’\n",
      "\n",
      "\n",
      "The following objects are masked from ‘package:tidyr’:\n",
      "\n",
      "    expand, pack, unpack\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "library(tidyverse)\n",
    "library(repr)\n",
    "library(readxl)\n",
    "library(infer)\n",
    "library(cowplot)\n",
    "library(GGally)\n",
    "library(broom)\n",
    "library(dplyr)\n",
    "library(car)\n",
    "library(tidymodels)\n",
    "library(glmnet)\n",
    "library(leaps)\n",
    "library(faraway)\n",
    "library(mltools)\n",
    "library(caret)\n",
    "library(forcats)\n",
    "library(gridExtra)\n",
    "library(patchwork)\n",
    "library(scales)\n",
    "library(utils)\n",
    "library(httr)\n",
    "library(Hmisc)\n",
    "library(psych)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3981881a-75a8-4739-ba22-d6ffb19e5614",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ONP_raw <- read.csv('https://raw.githubusercontent.com/jasonllai/Stat-301-project-t36/main/OnlineNewsPopularity.csv', header = TRUE, sep = \",\")\n",
    "head(ONP_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54dd14c-1ecb-4b0a-8fbf-056290a12cf3",
   "metadata": {
    "tags": []
   },
   "source": [
    "*Table 2.1 First 6 rows of raw data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c79d1b6-0432-40dd-9037-1d8a22335eda",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Select variables of interest\n",
    "selected_data <- ONP_raw %>%\n",
    "  select(3,4,8,10,12,28,57,45,46,47,48,51,54,58,61)\n",
    "\n",
    "head(selected_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a51d183-97f9-46b1-8f7f-73c775ceb760",
   "metadata": {},
   "source": [
    "*Table 2.2 First 6 rows of data interested*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d8be36-9198-4db5-81c9-5d8d2ad0141f",
   "metadata": {},
   "source": [
    "| Column | Variable Name | Type | Meaning |\n",
    "|--------|-----------------------|-----|----|\n",
    "| 1      | `n_tokens_title`        | type numeric `dbl`| Number of words in the title |\n",
    "| 2      | `n_tokens_content`      | type numeric `dbl`| Number of words in the content |\n",
    "| 3      | `num_hrefs`             | type numeric `dbl`| Number of links |\n",
    "| 4      | `num_imgs`              | type numeric `dbl`| Number of images |\n",
    "| 5     | `average_token_length`  | type numeric `dbl`| Average length of the words in the content |\n",
    "| 6     | `kw_avg_avg`            | type numeric `dbl`| Average number of shares (popularity) of the average keyword included in the article (keywords are similar to hashtags) |\n",
    "| 7     | `global_subjectivity`   | type numeric `dbl`| Text subjectivity |\n",
    "| 8     | `global_sentiment_polarity` | type numeric `dbl`| Text sentiment polarity |\n",
    "| 9     | `global_rate_positive_words` | type numeric `dbl`| Rate of positive words in the content |\n",
    "| 10     | `global_rate_negative_words` | type numeric `dbl`| Rate of negative words in the content |\n",
    "| 11     | `avg_positive_polarity` | type numeric `dbl`| Average polarity of positive words |\n",
    "| 12     | `avg_negative_polarity` | type numeric `dbl`| Average polarity of negative words |\n",
    "| 13     | `title_subjectivity`    | type numeric `dbl`| Title subjectivity |\n",
    "| 14     | `title_sentiment_polarity` | type numeric `dbl` | Title sentiment polarity |\n",
    "| 15     | `shares`                | type numeric `int`| Number of shares on Facebook, Twitter, and Flipboard [RESPONSE VARIABLE] |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07646410-704f-4312-9299-74ef297f5cc9",
   "metadata": {},
   "source": [
    "### Research question:\n",
    "How can we build an effective model to predict the number of shares an article gets based on the following variables: number of words in the title (`n_tokens_title`), number of images (`num_imgs`), number of words in content (`n_tokens_content`), number of links (`num_hrefs`), average length of words in the content (`average_token_length`), title subjectivity (`title_subjectivity`), global subjectivity (`global_subjectivity`), global sentiment polarity (`global_sentiment_polarity`), rate of positive words (`rate_positive_words`), rate of negative words (`rate_negative_words`), negative polarity (`avg_negative_polarity`), positive polarity (`avg_positive_polarity`), title sentiment polarity (`title_sentiment_polarity`), and average keywords (`kw_avg_avg`)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df30a0f-adb2-4a46-b893-a9218087e5bd",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Cleaning and Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab4811c-8854-4049-85b3-c58e662d20bc",
   "metadata": {},
   "source": [
    "#### Integer Values\n",
    "\n",
    "Most of our variables are integer values (counts) without decimals (such as number of images, words, links, etc.). We will check which variables need transformation using `sapply()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711f25af-a37a-4521-bcff-4aa845cea797",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Assuming df is your data frame\n",
    "result <- sapply(selected_data, function(x) all(x == floor(x)))\n",
    "                 \n",
    "# Convert integer columns\n",
    "for (col in names(selected_data)) {\n",
    "    if (result[col]) {\n",
    "        selected_data[[col]] <- as.integer(selected_data[[col]])\n",
    "    }\n",
    "}              \n",
    "str(selected_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61cb78ab-605e-48d9-8cbd-1f223a9aea6a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Readable column names\n",
    "news_data <- selected_data %>%\n",
    "    rename(n_words_title = n_tokens_title,\n",
    "           n_words_content = n_tokens_content,\n",
    "           n_links = num_hrefs,\n",
    "           n_images = num_imgs,\n",
    "           avg_word_length = average_token_length,\n",
    "           avg_keyword_popularity = kw_avg_avg,\n",
    "           subjectivity_title = title_subjectivity,\n",
    "           subjectivity_text = global_subjectivity,\n",
    "           sentiment_polarity_text = global_sentiment_polarity,\n",
    "           sentiment_polarity_title = title_sentiment_polarity,\n",
    "           rate_positive_words_text = global_rate_positive_words,\n",
    "           rate_negative_words_text = global_rate_negative_words,\n",
    "           avg_positive_polarity = avg_positive_polarity,\n",
    "           avg_negative_polarity = avg_negative_polarity\n",
    "          )\n",
    "str(news_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db47bb83-e8f8-467f-bb7f-7a0b4e562080",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Explanatory Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3833a2c7-d36a-4df1-b00a-5e046c790ad7",
   "metadata": {},
   "source": [
    "#### Basic Summary Statistics\n",
    "\n",
    "We will start by exploring the surface level characteristics of the data such as the variable types, number of rows, columns and check if the dataset contains any missing (NA) values.\n",
    "\n",
    "We will then use the `describe()` function to explore the basic summary statistics including measures of central tendency and measures of dispersion. This will give us a good grasp on where the center of the variables are located and how spread out the values are within those variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce77e6e8-b255-45bf-a587-09bd7e855f64",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dims <- dim(news_data)\n",
    "rows <- dims[1]\n",
    "columns <- dims[2]\n",
    "\n",
    "cat(\"\\nDimensions and missing values:\\n\")\n",
    "# Dimensions and NA check\n",
    "if (sum(is.na(news_data)) == 0){\n",
    "    print(paste0(\"The dataset consists of \", as.character(rows), \" observations and \",  as.character(columns), \" variables (\", \n",
    "                 as.character(columns-1), \" + 1 response variable). There are no missing values in the dataset.\"))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2eba454-544d-4461-a011-44592d5ee4a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cat(\"\\nBasic summary statistics table - Measures of Central Tendency and Measures of Dispersion:\\n\")\n",
    "description <- round(psych::describe(news_data), 2)\n",
    "\n",
    "description$variance <- description$sd^2\n",
    "\n",
    "round(description, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3445c5-fdca-42c6-b13e-50fe46173137",
   "metadata": {},
   "source": [
    "*Table 2.3 Measures of Central Tendency and Measures of Dispersion*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f4e357-b115-4b3a-9f92-0e6880ce2d5b",
   "metadata": {},
   "source": [
    "#### Observations:\n",
    "In _**Poisson regression**_ it's important that the mean equals the variance for the count data. The variance for `shares` is significantly greater than the mean. This is called _overdispersion_ and it needs to be investigated further. This can be due to the excess of zeros in these data or the nature of count (`int`) data.  \n",
    "\n",
    "Since some of the count variables can be true 0's, we will only focus on the `n_words_content` and `avg_word_length` variables. This is because we expect the articles we'll be using in the analysis to have at least some word content in them. We will check the articles with the `url` column in the original dataset. If they are geniune data we will keep the 0's, if not, we will remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e42b93-d62c-4a81-849b-5e477921c5d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Subset data of articles with suspicious word counts and their links\n",
    "no_word_articles <- ONP_raw %>% filter(n_tokens_content == 0 | average_token_length == 0) %>% select(url, shares)\n",
    "head(no_word_articles, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e69cf65-6944-4b9d-8014-aaf3afb4a3eb",
   "metadata": {},
   "source": [
    "*Table 2.4 Subset data of articles with suspicious word counts and their links*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd51a9a-46e8-4f1d-9eae-a095ae05a81b",
   "metadata": {},
   "source": [
    "After analyzing some of the articles, we found that they do have words in them (words not equal to 0) and thus, their average word lengths shouldn't be equal to 0. These are erroneous entries, and wil be removed from the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6981cc2-7eca-41e1-99b4-640e8d993f1c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Data entry errors removed\n",
    "news_data <- news_data %>% filter(n_words_content != 0, avg_word_length != 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1b4a35-9108-472d-bbb0-4394a2baefa6",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Correlation Matrix\n",
    "\n",
    "We will continue the EDA with a correlation matrix. This will help us have a better idea of both the correlations between variables and with the response variable `shares`. \n",
    "\n",
    "**NOTE:** Any correlation below 0.5 will be assigned 'NA' to highlight the notable correlations only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067b94a8-27cf-43e6-93a9-515728880a34",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create correlation matrix\n",
    "cor_matrix <- cor(news_data[,-1])\n",
    "threshold <- 0.5\n",
    "\n",
    "cor_matrix[abs(cor_matrix) < threshold] <- NA\n",
    "round(cor_matrix, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8bf3e9-87fb-4bb7-a0af-02c8ec845481",
   "metadata": {},
   "source": [
    "*Table 2.5 correlation matrix*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ddcd417-a3b9-4e81-9722-98ffd6c099d4",
   "metadata": {},
   "source": [
    "#### Variance Inflation Factor (VIF) Analysis\n",
    "\n",
    "Since Poisson Regression doesn't account for multicollinearity, we will do a VIF analysis to detect if any correlations are problematic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55809523-5117-497c-9df7-64564a6c262a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lm_vif_model <- lm(shares ~., data = news_data)\n",
    "\n",
    "vif <- vif(lm_vif_model)\n",
    "\n",
    "print(round(vif, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0d8864-2f42-4404-8049-193a01d9a2b0",
   "metadata": {
    "tags": []
   },
   "source": [
    "We observe that only the `sentiment_polarity_text` can be considered problematic, with a **VIF score of  6.508**. We will drop this variable before continuing the analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e926299-8b7c-4297-b0d3-d96318081c79",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "news_data <- news_data %>% select(-sentiment_polarity_text )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b006197-8446-4c90-b5e3-a84ffec2273b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lm_vif_model2 <- lm(shares ~., data = news_data)\n",
    "\n",
    "vif2 <- vif(lm_vif_model2)\n",
    "\n",
    "print(round(vif2, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8ebd03-90f5-470d-b25c-b897ae56f9b4",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Visualizations\n",
    "\n",
    "Next up, we will use `ggplot2::ggpairs()` function to visualize these relationships using correlations and scatterplots for continuous variables. \n",
    "\n",
    "Due to the heavy skew observed in the summary statistics (**_skew = 33.96_**) and the extremely large range of the `shares` varaible (**_range = 843299_**) given the relatively small mean of this variable (**_mean = 3395.38_**), we will first filter the number of shares to less than 20,000. This will help viusalize the response variable more clearly. \n",
    "\n",
    "Trimmed dataset where the `shares` variable is trimmed to remove outliers will be assigned to a new variable called `news_data_trim`.\n",
    "\n",
    "We will take a sample of 250 using `sample_n()` for computational purposes and we will use `set.seed()` function for reproducibility, before visualizing with `ggpairs()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d935e1-4c1d-4b38-a448-ce9042ecdce1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "set.seed(123)\n",
    "# Trim response variable\n",
    "news_data_trim <- news_data %>% filter(shares < 20000)\n",
    "\n",
    "# Sample for correlations\n",
    "sample_250 <- news_data_trim %>% sample_n(250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282a68c7-2c54-4b2b-ab6f-852d1d94fd24",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ggpairs(sample_250[,c(1:5, 14)], progress = FALSE,\n",
    "        lower = list(continuous = wrap(\"points\", alpha = 0.2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fefcd04c-bc02-42d3-b911-0ebcff0e6466",
   "metadata": {},
   "source": [
    "*Figure 2.1 First subset of correlation matrix*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a668bd-9114-4801-9130-215c3d7d0814",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ggpairs(sample_250[,c(6:14)], progress = FALSE,\n",
    "       lower = list(continuous = wrap(\"points\", alpha = 0.1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8703beb8-066d-4b84-94a4-11adc7bae2e5",
   "metadata": {},
   "source": [
    "*Figure 2.2 Second subset of correlation matrix*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feda1e94-cbbf-4f46-91a2-91e66a0bea66",
   "metadata": {},
   "source": [
    "### Histogram Plots\n",
    "\n",
    "We now want to have a look at the distribution of the response variable `shares`. We use geom_histogram to plot the distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31234e32-4668-426a-b532-a7a85512ad63",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Sample for correlations\n",
    "sample_1000 <- news_data_trim %>% sample_n(1000)\n",
    "\n",
    "options(repr.plot.width = 8, repr.plot.height = 4)\n",
    "shares_dist <- sample_1000 %>%\n",
    "    ggplot(aes(x = shares)) +\n",
    "    geom_histogram(bins = 50, color = \"white\") +\n",
    "    xlim(0,20000) +\n",
    "    labs(x = \"Number of shares\", y = \"Count\") +\n",
    "    ggtitle(\"Distribution of Number of shares\") +\n",
    "    theme(text = element_text(size = 10)) +\n",
    "    theme(plot.title = element_text(hjust = 0.45))\n",
    "shares_dist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e5d27f-32c8-4b33-8af6-66f3ecde5c74",
   "metadata": {},
   "source": [
    "*Figure 2.3 Distribution of Number of shares*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11967c5e-968c-4198-8972-29c4348a0206",
   "metadata": {},
   "source": [
    "This graph shows a heavily right skewed distribution of our interested response variable (`shares`). \n",
    "\n",
    "We noticed that there are 2 rows removed from this graph, this is because these data points are lied on the very right of the hostogram and their counts are very low. Since the grpah is to visualize the distribution of number of shares, we can temporarily ignore them here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a79c600-68ab-426f-8e4c-377647c321bb",
   "metadata": {},
   "source": [
    "## Model Selection and Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1be720-52cd-41f5-8f81-d65c7d653fe4",
   "metadata": {},
   "source": [
    "In this project, we want to compare two models: `Ridge Regression` and `Poisson Regression`. We will perform cross-validation to compare two models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65760538-bdb5-4450-9da8-dcc63f177fdf",
   "metadata": {},
   "source": [
    "### Splitting Training Data and Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df39112-35e8-47d3-9722-0e33838f93ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "news_data <- news_data %>% relocate(shares)\n",
    "\n",
    "# Split data into training and test sets\n",
    "set.seed(123)\n",
    "\n",
    "training_ONP  = news_data %>%\n",
    "  sample_frac(0.6)\n",
    "\n",
    "testing_ONP = news_data %>%\n",
    "  setdiff(training_ONP)\n",
    "\n",
    "head(training_ONP,6)\n",
    "head(testing_ONP,6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd609a0c-5153-4c38-a611-33be5a33f990",
   "metadata": {},
   "source": [
    "*Table 2.6 First six rows for training set and testing set*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c667623-f20a-401e-ba21-f796b37428bf",
   "metadata": {},
   "source": [
    "### Building Ridge Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7fdafc-7ff4-439c-a885-367a5798641d",
   "metadata": {},
   "source": [
    "To train our Ridge Regression model, we will use `glmnet` package, which requires a matrix with input variables and a vector of responses. Thus, we use as.matrix function to prepare the model matrix for glmnet. We create 4 matrices, which are the training x-matrix `ONP_X_train` and training y-matrix `ONP_Y_train`; the testing x-matrix `ONP_X_test` and testing y-matrix `ONP_Y_test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f982e3-5e54-4d1c-b3d8-64efc9f4508a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build matrix and vector required by `glmnet`\n",
    "\n",
    "ONP_X_train <- model.matrix(object = shares ~ .,\n",
    "  data = training_ONP)[, -1]\n",
    "\n",
    "ONP_Y_train <- training_ONP[, \"shares\"]\n",
    "\n",
    "ONP_X_test <- model.matrix(object = shares ~ .,\n",
    "  data = testing_ONP)[, -1]\n",
    "\n",
    "ONP_Y_test <- testing_ONP[, \"shares\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8efac2-1165-40cc-a48a-3428808723c4",
   "metadata": {},
   "source": [
    "With the training data `ONP_X_train` and `ONP_Y_train`, we will use `cv.glmnet()` function to find an \"optimal\" value of $\\hat{\\lambda}$ value for our Ridge model. In `cv.glmnet()` function, we set `alpha = 0` as we want to get a Ridge model. We want to see the results and find the value $\\hat{\\lambda}_{min}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a05126-59e8-46da-a482-70723cec1000",
   "metadata": {},
   "outputs": [],
   "source": [
    "ONP_cv_lambda_ridge <- cv.glmnet(\n",
    "  x = ONP_X_train, y = ONP_Y_train,\n",
    "  alpha = 0,\n",
    "  lambda = exp(seq(-5, 10, 0.1))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418defa1-7fca-4605-bb0a-7737d16a8a1a",
   "metadata": {},
   "source": [
    "Now, we will examine the result of the cross-validation `ONP_cv_lambda_ridge`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a0aa19-b11a-4047-9c51-6a1a2864e92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ONP_cv_lambda_ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9416e6-409c-40cb-85cc-a2613ea2ef6d",
   "metadata": {},
   "source": [
    "We can see that $\\hat{\\lambda}_{\\text{1SE}}$ here is very large, which slightly implies that Ridge Regression might not be very suitable for our data. However, we will use $\\hat{\\lambda}_{min}$ to build our Ridge Model and name it `ONP_cv_lambda_ridge`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4eecb3-351d-47fa-bfe7-4f6b84439ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ONP_cv_lambda_ridge <- glmnet(\n",
    "  x = ONP_X_train, y = ONP_Y_train,\n",
    "  alpha = 0,\n",
    "  lambda = ONP_cv_lambda_ridge$lambda.min\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb0e92d-9f92-4792-8440-2a79e33a49a8",
   "metadata": {},
   "source": [
    "### Building Poisson Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb6815d-d840-4fe5-944d-3827372d7088",
   "metadata": {},
   "source": [
    "The reason we build Poisson Regression Model is because a Poisson random variable takes discrete non-negative integer values that count something in a given timeframe. Our dataset has number of shares as response variable which is also a discrete non-negative variable counting the number of shares in a period of 2 years. This characteristic of our response variable meets all the traits of a Poisson random variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e50cac-ee0f-43e3-a700-91fbc81e89ab",
   "metadata": {},
   "source": [
    "In order to fit a Poisson regression model, we can use the function `glm()` and its argument `family = poisson`, which obtains the estimates $\\hat{\\beta}_0, \\hat{\\beta}_1, \\dots \\hat{\\beta}_{p}$. The estimates are obtained through maximum likelihood where we assume a Poisson joint probability mass function of the $n$ responses $Y_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701c1731-d542-43f7-a71a-41f1089bd39a",
   "metadata": {},
   "source": [
    "Here we use `shares` as response variable and all other variables as explanatory variables to build a Poisson regression model, name it `ONP_Poisson_model`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f0aab8-e864-4e46-99ce-8008672204b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ONP_Poisson_model <- glm(\n",
    "  formula = shares ~.,\n",
    "  data = training_ONP,\n",
    "  family = poisson\n",
    ")\n",
    "summary(ONP_Poisson_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358adf50-dc18-475d-9c77-d8b4874319d5",
   "metadata": {},
   "source": [
    "### Model Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75f31df-8e42-405c-a635-8f26f7f69546",
   "metadata": {},
   "source": [
    "To select the best model for the dataset to predict number of shares, we use `caret` package with `glmnet` to perform a 10-fold CV for comparing models. Below is the specific code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0592d7e1-5f08-4492-997a-b46d297abfb4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 1: Prepare data\n",
    "predictors <- subset(training_ONP, select = -c(shares)) # Features\n",
    "target <- training_ONP$shares # Target variable\n",
    "\n",
    "# Step 2: Create the models (Ridge regression and Poisson regression)\n",
    "# Ridge regression using glmnet package\n",
    "model_ridge <- train(predictors, target, method = \"glmnet\", \n",
    "                     trControl = trainControl(method = \"cv\", number = 10),\n",
    "                     tuneGrid = expand.grid(alpha = 0, lambda = seq(-5, 10, by = 0.1)), \n",
    "                     family = \"gaussian\")\n",
    "\n",
    "# Poisson regression using glm package\n",
    "model_poisson <- train(predictors, target, method = \"glm\", \n",
    "                       trControl = trainControl(method = \"cv\", number = 10),\n",
    "                       tuneLength = 5,\n",
    "                       family = poisson())\n",
    "\n",
    "# Step 3: Perform cross-validation to compare models\n",
    "compareModels <- resamples(list(Ridge = model_ridge, Poisson = model_poisson))\n",
    "\n",
    "# Summarize and compare the performance of models\n",
    "summary(compareModels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d51fa87-6650-47ec-ae18-91fa830468ad",
   "metadata": {
    "tags": []
   },
   "source": [
    "From the summary above, $MAE$, $RMSE$, and $R^2$ values are calculated for both models. We will focus on analyzing the mean for each metrics for both models. We can see that the $MAE$ for Ridge Regression Model is slightly lower than($\\approx$ 1) $MAE$ of Poisson Regression Model. The $RMSE$ for Ridge Regression Model is a lot higher than ($\\approx$ 3687) $RMSE$ of Poisson Regression Model. The $R^2$ for Ridge Regression Model is slightly higher than($\\approx$ 0.002) $R^2$ of Poisson Regression Model. Since the $MAE$ and $R^2$ metric values for both models are about the same whereas the $RMSE$ of Poisson Regression Model is much less than that of Ridge Regression Model, **we can conclude that Poisson Regression Model is better for us to use to predict number of shares in a period of two years.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ccd38f-3dc4-4dfa-bafb-6f1c1229bbc0",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ed3609-4c2d-48b1-b444-5f72cd61f143",
   "metadata": {},
   "source": [
    "We use `predict()` function and the Poisson regression model `ONP_Poisson_model` to obtain the out-of-sample predicted values of number of shares of an article in a period of two years using testing set `testing_ONP`. We store the predicted values in a variable called `test_pred_poisson`. We will look at the first few predicted value using `head()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8600a1c5-095a-4c68-a951-ace3afaa7ceb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_pred_poisson <- predict(ONP_Poisson_model, newdata = testing_ONP,\n",
    "                             type = \"response\")\n",
    "head(test_pred_poisson)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae8b87b-7087-4537-94fd-f0374d409030",
   "metadata": {},
   "source": [
    "Use the function `rmse()` to compute the  RMSE$_{test}$ using the predicted values stored in `test_pred_poisson`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9af618-da95-437d-be8b-72d4d5e1383f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ONP_R_MSE_models <- tibble(\n",
    "  Model = \"Poisson Regression\",\n",
    "  R_MSE = rmse(\n",
    "    preds = test_pred_poisson,\n",
    "    actuals = testing_ONP$shares\n",
    "  )\n",
    ")\n",
    "ONP_R_MSE_models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296f3805-4bcc-42d6-9662-8ccffd60622d",
   "metadata": {},
   "source": [
    "*Table 2.7 RMSE of predicted values by Poisson Regression*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b87a7c-72db-4222-98b9-7d118b0add12",
   "metadata": {},
   "source": [
    "The RMSE value above indicates that the average difference between the number of shares predicted by Poisson model and the actual number of shares is approximately 12582. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c93070e-e5ac-405f-8558-08c3f679de9f",
   "metadata": {},
   "source": [
    "# Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e978f5f5-db28-464d-a1cb-b58cf95c02ed",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "In our analysis, we explored Ridge Regression and Poisson Regression. Ridge Regression uses an L2-norm to measure the size of the coefficients, which is the sum of the squares of each coefficient. It is appropriate because our dataset has many explanatory variables to consider, but none of them are strongly correlated with the response variable. Poisson Regression is also an appropriate model to use because our response variable is a count. This method takes discrete non-negative integer values, and the Poisson distribution has a mean equal to its variance.\n",
    "\n",
    "We first checked the **Linearity assumption** by plotting the residuals and observing randomness. Next, we ensured that all errors were independent to fulfill the **Independence assumption**. After that, we checked the **Constant Variance assumption**, which assumes that the errors have equal variance. Randomness in the residuals-fitted value plot indicates that heteroskedasticity is not present. One major issue we expected to face was *multicollinearity* because there are many explanatory variables, some of which could be related. We used a correlation matrix and VIF to quantify multicollinearity, and dropped the variable sentiment_polarity_text to continue with our analysis. Furthermore, there are no missing values in the dataset. The distribution of shares had a heavy right-skew, so we concluded that articles with more than 20,000 shares were outliers and safely ignored them.\n",
    "\n",
    "Although Ridge can be used to address multicollinearity problems, one limitation is that it does not select variables since the estimated coefficients won't be shrunk to zero. Another limitation is that the coefficients may be biased. However, this is not a major concern because having biased coefficients does not affect our predictions. Our lambda.min value of 1097 gives the value with the minimum mean cross-validation error. Our lambda.1se value of 22026 gives the value such that the cross-validation error is within one standard error of the minimum, which is about 10 times larger than the lambda.min value. This corresponds with a higher level of penalization, and may imply that Ridge Regression might not be very suitable for our data.\n",
    "\n",
    "For Poisson Regression, any factor that affects the mean will also affect the variance, which could be a potential drawback for using this model. Residual deviance measures how much probabilities estimated from our model differ from the observed proportions of successes. In our case, the value is 135116829, which suggests a very large difference. This means the Poisson method may not produce the best model either."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a3a6d4-4568-430c-8bac-cae3f57b69c4",
   "metadata": {},
   "source": [
    "### Key Findings\n",
    "\n",
    "After performing a 10-fold cross validation to compare Ridge and Poisson Regression Models, we concluded that the **Poisson Regression Model is better to predict article shares.** However, the Poisson Regression’s RMSE value based on the test dataset produced a value of 12582, which is relatively large for the difference between the number of shares. This means that the **Poisson Regression model may not be the best overall model to analyze this dataset.**\n",
    "\n",
    "\n",
    "### Improvements and Future Research\n",
    "\n",
    "Although it is beyond the scope of STAT 301, a **Negative Binomial Regression model** may produce a better model for our dataset because it is more sensitive to heavy skew in the data. Also, we could have used other proportions (0.5, 0.7, etc.) to split the data and see which proportion gives us the best model. A future question we have is whether we can generalize this prediction model. For instance, can this model be applied to articles from other news sites, such as the New York Times or Washington Post? Can it be generalized to other digital platforms, including Instagram and Reddit? Other sites and platforms may affect how articles are shared, so more research and analysis are needed to determine external validity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4143b106-9fc7-4612-b128-7c1c0812cfb9",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db9e3bf-9ef9-4e7f-baad-2716d5d67999",
   "metadata": {},
   "source": [
    "Trilling, D., Tolochko, P., & Burscher, B. (2017). From Newsworthiness to Shareworthiness: \n",
    "How to Predict News Sharing Based on Article Characteristics. Journalism & Mass \n",
    "Communication Quarterly, 94(1), 38-60. https://doi.org/10.1177/1077699016654682\n",
    "\n",
    "Fernandes, K., Vinagre, P., Cortez, P., & Sernadela, P. (2015, May 30). Online News Popularity. UCI Machine Learning Repository. https://archive.ics.uci.edu/dataset/332/online+news+popularity\n",
    "\n",
    "Liedke, J., & Wang, L. (2023, November 15). News Platform Fact Sheet. Pew Research Center’s Journalism Project. https://www.pewresearch.org/journalism/fact-sheet/news-platform-fact-sheet/#:~:text=The%20transition%20of%20the%20news,they%20are%20getting%20their%20news. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.2.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
